{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurement IP Data Downloader\n",
    "__authors__: ufukbombar@gmail.com\n",
    "\n",
    "__date__: 2024-11-24\n",
    "\n",
    "__goal__: This notebook downloads the ip routing information into binary files given the measurement id. The format is given.\n",
    "\n",
    "__input__:\n",
    " - The measurement table as .feather file from `table_to_measurement_converter`.\n",
    " - The measurement_uuid given by the user.\n",
    "\n",
    " __output__:\n",
    "  - The binary data for each agent under the data dir.\n",
    "\n",
    "__notes__:\n",
    "The only required fields for finding out the routing is ['probe_dst_prefix', 'reply_src_prefix']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "%pip install pandas requests python-dotenv httpx aiofiles dataclasses tqdm pyarrow\n",
    "\n",
    "# External imports\n",
    "import pandas as pd \n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import os\n",
    "import ipaddress\n",
    "\n",
    "# Internal imports\n",
    "from config import config, Config\n",
    "import common\n",
    "import file as ff\n",
    "import objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the output of table_downloader\n",
    "input_tables_name = \"../data/measurements/measurement-2024-11-18 22:44:16.feather\"\n",
    "selected_measurement_uuid = \"007046a9_518e_46cb_8e70_e598b8bce831\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_tables_df = pd.read_feather(input_tables_name)\n",
    "measurement_tables_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_measurements_df = measurement_tables_df[\n",
    "    (measurement_tables_df['cleaned'] == True) & \n",
    "    (measurement_tables_df['measurement_uuid'] == selected_measurement_uuid) & \n",
    "    (measurement_tables_df['type'] == 'results')\n",
    "]\n",
    "selected_measurements_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the df to list[dataclass: Measurement]\n",
    "selected_measurements_list = [objects.Measurement(**row) for _, row in selected_measurements_df.iterrows()]\n",
    "print(f\"There are {len(selected_measurements_list)} table(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_number_of_row(m: objects.Measurement): \n",
    "    q_size = f\"select count(*) from {m.table_name}\"\n",
    "    cfg = Config()\n",
    "    cfg.response_format = \"CSV\"\n",
    "    raw_response = await common.quick_query(cfg, q_size)\n",
    "    return int(raw_response.decode())\n",
    "\n",
    "async def get_results_table_optimized(m: objects.Measurement, num_rows: int, limit: int=1, pbar: tqdm=None, pbar_update=100):\n",
    "    # Define the packing/compression function\n",
    "    def from_csv_to_int(binary_data: str) -> tuple[bytes, bool]:\n",
    "        if not isinstance(binary_data, str):\n",
    "            raise Exception(\"Given wrong type!\")\n",
    "        \n",
    "        a, b = binary_data.split(',')\n",
    "        a, b = a[1:-1], b[1:-1]\n",
    "        a, b = ipaddress.IPv6Address(a).ipv4_mapped, ipaddress.IPv6Address(b).ipv4_mapped\n",
    "\n",
    "        if a == None or b == None:\n",
    "            return b'', False\n",
    "\n",
    "        # remove the last byte since it is a prefix. The reouting information is represented with \n",
    "        # 3 + 3 = 6 bytes.\n",
    "        return a.packed[:-1] + b.packed[:-1], True\n",
    "\n",
    "    # Get the default config\n",
    "    cfg = Config()\n",
    "    cfg.response_format = \"CSV\"\n",
    "\n",
    "    # Set the quert string\n",
    "    q = f\"select probe_dst_prefix, reply_src_prefix from {m.table_name}{'' if limit == 0 else ' limit ' + str(limit)}\"\n",
    "\n",
    "    # Determine the name of the binary data\n",
    "    filepath = f\"../data/ip_data/{m.measurement_uuid}-{ff.get_timestr()}/{m.table_name}.bin\"\n",
    "    num_pairs_written = 0\n",
    "    num_pairs_expected = num_rows\n",
    "    \n",
    "    try:\n",
    "        # with ff.BinaryFile(filepath) as bf:\n",
    "        async for row in common.run_query_iter_lines(cfg, q):\n",
    "            b, ok = from_csv_to_int(row)\n",
    "            if not ok: continue\n",
    "            num_pairs_written += 1\n",
    "\n",
    "            # pbar.update(6 / (1024*1024)) # one row is 6 bytes\n",
    "            pbar.update(1)\n",
    "\n",
    "                # bf.write(b) # writes 6 bytes each time.\n",
    "    except Exception as e:\n",
    "        print(f\"Exception on {m.table_name}, exitting\")\n",
    "        raise e\n",
    "        return (False, num_pairs_expected, num_pairs_written)\n",
    "\n",
    "    return (True, num_pairs_expected, num_pairs_written)\n",
    "\n",
    "async def get_results_table_optimized_from_list(m_list: list[objects.Measurement], limit: int=1):\n",
    "    num_rows_list = await asyncio.gather(*[get_number_of_row(m) for m in m_list])\n",
    "    if limit == 0:\n",
    "        num_rows_c = sum(num_rows_list)\n",
    "        num_rows_tqdm = num_rows_c\n",
    "    else:\n",
    "        num_rows_c = limit\n",
    "        num_rows_tqdm = limit * len(m_list)\n",
    "\n",
    "    # One row is 6 bytes\n",
    "    # Bytes\n",
    "    total_mb = round((6 * num_rows_c) / (1024*1024), ndigits=2) + 1\n",
    "    print(f\"Will download total of {num_rows_c} row(s) and {total_mb} MB.\")\n",
    "\n",
    "    # pbar = tqdm(total=num_rows_tqdm, desc=\"Downloading\", ncols=200)\n",
    "    # r = await asyncio.gather(*[get_results_table_optimized(m, nr, limit=limit, pbar=pbar) for m, nr in zip(m_list, num_rows_list)])\n",
    "    # pbar.close()\n",
    "    # return r\n",
    "\n",
    "# ok_expected_writted_list = await get_results_table_optimized_from_list(selected_measurements_list, limit=0)\n",
    "\n",
    "# for m, ok_exp_writ in zip(selected_measurements_list, ok_expected_writted_list):\n",
    "#     ok, expected, written = ok_exp_writ\n",
    "#     print(f\"For agent {m.agent_uuid} {written}/{expected} pairs are saved with {'success' if ok else 'error'}\")\n",
    "\n",
    "# await get_results_table_optimized(selected_measurements_list[0], 1, limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_number_of_pairs = sum(await asyncio.gather(*[get_number_of_row(m) for m in selected_measurements_list]))\n",
    "total_number_of_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_number_of_pairs_per_table = total_number_of_pairs // len(selected_measurements_list)\n",
    "avg_number_of_pairs_per_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for ip conversion -> IT WORKS\n",
    "def from_bin_to_ipv4(binary_data: bytes):\n",
    "    a, b = binary_data[:3] + b'\\00', binary_data[3:] + b'\\00'\n",
    "    return ipaddress.ip_address(a), ipaddress.ip_address(b)\n",
    "\n",
    "from_bin_to_ipv4(b'\\x01\\x00\\xd5\\xcb\\xbe\\xfa') # expected (\"::ffff:1.0.213.0\",\"::ffff:203.190.250.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def nested_task(task_id, total_steps):\n",
    "    # Create a nested progress bar for each task\n",
    "    for i in tqdm(range(total_steps), desc=f\"Subtask {task_id}\", leave=False, ncols=100):\n",
    "        await asyncio.sleep(0.1)  # Simulate async work\n",
    "\n",
    "async def main_task(task_id, total_subtasks, subtask_steps):\n",
    "    # Create a progress bar for the main task\n",
    "    for i in tqdm(range(total_subtasks), desc=f\"Main Task {task_id}\", ncols=100):\n",
    "        # Simulate async work for the main task\n",
    "        await asyncio.sleep(0.2)\n",
    "        \n",
    "        # Call the nested task and await it\n",
    "        await nested_task(i, subtask_steps)\n",
    "\n",
    "async def main():\n",
    "    total_tasks = 3  # Total number of main tasks\n",
    "    total_subtasks = 5  # Subtasks per main task\n",
    "    subtask_steps = 4  # Steps per nested subtask\n",
    "\n",
    "    tasks = []\n",
    "    for i in range(total_tasks):\n",
    "        tasks.append(main_task(i, total_subtasks, subtask_steps))\n",
    "\n",
    "    # Run all tasks concurrently\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "# Run the main async function\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\x01\\x02\\xf1\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xb4\\xb4\\xf8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\x01\\x02\\xf1\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xb4\\xb4\\xf8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\x01\\x02\\xf1\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xb4\\xb4\\xf8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\x01\\x02\\xf1\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xb4\\xb4\\xf8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\x01\\x02\\xf1\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xb4\\xb4\\xf8\\x00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b)\n",
    "b[::8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"aAbBcCdDeEfFgGhH\"\n",
    "row_size = 2\n",
    "offset = 0\n",
    "\n",
    "for i in range(len(a)):\n",
    "    print(a[offset:offset + row_size])\n",
    "    offset += row_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import ipaddress\n",
    "\n",
    "def bytes_consumer(buffer_bytes: bytes):\n",
    "    row_size = 32\n",
    "    ipv6_size = 16\n",
    "    new_row_size = 6\n",
    "    num_pairs = len(buffer_bytes) // row_size # each row is 32 bytes\n",
    "    new_buffer_size = num_pairs * new_row_size # Each row will be 6 bytes in the new one\n",
    "\n",
    "    new_buffer = io.BytesIO()\n",
    "    new_buffer.truncate(new_buffer_size)\n",
    "\n",
    "    offset = 0\n",
    "    for _ in tqdm(range(num_pairs)):\n",
    "        row = buffer_bytes[offset: offset + row_size]\n",
    "        \n",
    "        first = buffer_bytes[offset:offset + ipv6_size]\n",
    "        second = buffer_bytes[offset + ipv6_size:offset + row_size]\n",
    "\n",
    "        first_address = ipaddress.ip_address(first).ipv4_mapped\n",
    "        second_address = ipaddress.ip_address(second).ipv4_mapped\n",
    "\n",
    "        if first_address == None or second_address == None:\n",
    "            print(\"Invalid IPv6 Addresses, this might be a problem.\")\n",
    "\n",
    "        new_buffer.write(first_address.packed[:-1])\n",
    "        new_buffer.write(second_address.packed[:-1])\n",
    "\n",
    "        offset += row_size\n",
    "\n",
    "    new_buffer.seek(0)\n",
    "    return new_buffer\n",
    "\n",
    "\n",
    "num_in_long_bytes = 75_830_108 # average number of paris per agent\n",
    "num_in_long_bytes = 1_000_000\n",
    "long_bytes = b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\x01\\x02\\xf1\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xff\\xb4\\xb4\\xf8\\x00' * num_in_long_bytes\n",
    "buf = bytes_consumer(long_bytes)\n",
    "bytes_from_buffer = buf.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_addresses = [ipaddress.ip_address(ip) for ip in bytes_from_buffer[:32*3:3]]\n",
    "ip_addresses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cfg = Config()\n",
    "test_cfg.response_format = \"RowBinary\"\n",
    "await common.quick_query(test_cfg, 'with [1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] as ii select ii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
